{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f832a8c6-bc32-4c0b-9465-b48f7ae4b1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: PyPDF2 in ./.local/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: shiba-model in ./.local/lib/python3.10/site-packages (0.1.1)\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (from shiba-model) (2.5.0)\n",
      "Requirement already satisfied: local-attention==1.4.1 in ./.local/lib/python3.10/site-packages (from shiba-model) (1.4.1)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from torch->shiba-model) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/tljh/user/lib/python3.10/site-packages (from torch->shiba-model) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.10/site-packages (from torch->shiba-model) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /opt/tljh/user/lib/python3.10/site-packages (from torch->shiba-model) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.10/site-packages (from torch->shiba-model) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch->shiba-model) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch->shiba-model) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch->shiba-model) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.10/site-packages (from torch->shiba-model) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.local/lib/python3.10/site-packages (from torch->shiba-model) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.local/lib/python3.10/site-packages (from torch->shiba-model) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.local/lib/python3.10/site-packages (from torch->shiba-model) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.local/lib/python3.10/site-packages (from torch->shiba-model) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.local/lib/python3.10/site-packages (from torch->shiba-model) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.10/site-packages (from torch->shiba-model) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch->shiba-model) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch->shiba-model) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.local/lib/python3.10/site-packages (from torch->shiba-model) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.10/site-packages (from torch->shiba-model) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy==1.13.1->torch->shiba-model) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/tljh/user/lib/python3.10/site-packages (from jinja2->torch->shiba-model) (2.1.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2 shiba-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b358248-d232-4598-a532-acb6bc1d31ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 encoding result: {'input_ids': tensor([[57344,    83,    97,  ...,    32,    68,   111]]), 'attention_mask': tensor([[False, False, False,  ..., False, False, False]])}\n",
      "Chunk 1 encoding result: {'input_ids': tensor([[57344,   110,   101,  ...,    97,   109,    46]]), 'attention_mask': tensor([[False, False, False,  ..., False, False, False]])}\n",
      "Decoded Text: Sample PDFThis is a simple PDF ﬁle. Fun fun fun.Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Phasellus facilisis odio sed mi. Curabitur suscipit. Nullam vel nisi. Etiam semper ipsum ut lectus. Proin aliquam, erat eget pharetra commodo, eros mi condimentum quam, sed commodo justo quam ut velit. Integer a erat. Cras laoreet ligula cursus enim. Aenean scelerisque velit et tellus. Vestibulum dictum aliquet sem. Nulla facilisi. Vestibulum accumsan ante vitae elit. Nulla erat dolor, blandit in, rutrum quis, semper pulvinar, enim. Nullam varius congue risus. Vivamus sollicitudin, metus ut interdum eleifend, nisi tellus pellentesque elit, tristique accumsan eros quam et risus. Suspendisse libero odio, mattis sit amet, aliquet eget, hendrerit vel, nulla. Sed vitae augue. Aliquam erat volutpat. Aliquam feugiat vulputate nisl. Suspendisse quis nulla pretium ante pretium mollis. Proin velit ligula, sagittis at, egestas a, pulvinar quis, nisl.Pellentesque sit amet lectus. Praesent pulvinar, nunc quis iaculis sagittis, justo quam lobortis tortor, sed vestibulum dui metus venenatis est. Nunc cursus ligula. Nulla facilisi. Phasellus ullamcorper consectetuer ante. Duis tincidunt, urna id condimentum luctus, nibh ante vulputate sapien, id sagittis massa orci ut enim. Pellentesque vestibulum convallis sem. Nulla consequat quam ut nisl. Nullam est. Curabitur tincidunt dapibus lorem. Proin velit turpis, scelerisque sit amet, iaculis nec, rhoncus ac, ipsum. Phasellus lorem arcu, feugiat eu, gravida eu, consequat molestie, ipsum. Nullam vel est ut ipsum volutpat feugiat. Aenean pellentesque.In mauris. Pellentesque dui nisi, iaculis eu, rhoncus in, venenatis ac, ante. Ut odio justo, scelerisque vel, facilisis non, commodo a, pede. Cras nec massa sit amet tortor volutpat varius. Donec lacinia, neque a luctus aliquet, pede massa imperdiet ante, at varius lorem pede sed sapien. Fusce erat nibh, aliquet in, eleifend eget, commodo eget, erat. Fusce consectetuer. Cras risus tortor, porttitor nec, tristique sed, convallis semper, eros. Fusce vulputate ipsum a mauris. Phasellus mollis. Curabitur sed urna. Aliquam nec sapien non nibh pulvinar convallis. Vivamus facilisis augue quis quam. Proin cursus aliquet metus. Suspendisse lacinia. Nulla at tellus ac turpis eleifend scelerisque. Maecenas a pede vitae enim commodo interdum. Donec odio. Sed sollicitudin dui vitae justo.Morbi elit nunc, facilisis a, mollis a, molestie at, lectus. Suspendisse eget mauris eu tellus molestie cursus. Duis ut magna at justo dignissim condimentum. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Vivamus varius. Ut sit amet diam suscipit mauris ornare aliquam. Sed varius. Duis arcu. Etiam tristique massa eget dui. Phasellus congue. Aenean est erat, tincidunt eget, venenatis quis, commodo at, quam.\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "from shiba import Shiba, CodepointTokenizer, get_pretrained_from_hub\n",
    "\n",
    "# Load SHIBA model and tokenizer\n",
    "shiba_model = Shiba()\n",
    "shiba_model.load_state_dict(get_pretrained_from_hub())\n",
    "shiba_model.eval()  # Disable dropout\n",
    "tokenizer = CodepointTokenizer()\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a given PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def split_text(text, max_length=1800):\n",
    "    \"\"\"Split text into chunks within the max length allowed by SHIBA.\"\"\"\n",
    "    return [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
    "\n",
    "def process_pdf_with_shiba(pdf_path):\n",
    "    # Extract and clean text from PDF\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    # Split text into manageable chunks\n",
    "    text_chunks = split_text(text)\n",
    "    all_outputs = []\n",
    "    encoded_chunks = []\n",
    "\n",
    "    for i, chunk in enumerate(text_chunks):\n",
    "        # Encode each chunk\n",
    "        encoded = tokenizer.encode_batch([chunk])\n",
    "        print(f\"Chunk {i} encoding result:\", encoded)  # Debugging statement\n",
    "\n",
    "        # Access `input_ids` and `attention_mask` and structure inputs for the model\n",
    "        if 'input_ids' in encoded and 'attention_mask' in encoded:\n",
    "            input_ids = encoded['input_ids']\n",
    "            attention_mask = encoded['attention_mask']\n",
    "            inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "            outputs = shiba_model(**inputs)\n",
    "            all_outputs.append(outputs)\n",
    "            encoded_chunks.append(input_ids[0])  # Store input_ids for decoding\n",
    "\n",
    "    return all_outputs, encoded_chunks\n",
    "\n",
    "def decode_tokens(encoded_chunks):\n",
    "    decoded_text = \"\"\n",
    "    for tokens in encoded_chunks:\n",
    "        decoded_text += tokenizer.decode(tokens)\n",
    "    return decoded_text\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"sample.pdf\"\n",
    "outputs, encoded_chunks = process_pdf_with_shiba(pdf_path)\n",
    "decoded_text = decode_tokens(encoded_chunks)\n",
    "print(\"Decoded Text:\", decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6b0ce1d-7268-4471-ba77-126d3e355561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fpdf in ./.local/lib/python3.10/site-packages (1.7.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e95c74fd-b200-4bf8-bd63-8db35cf66964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-arnav/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "/home/jupyter-arnav/.local/lib/python3.10/site-packages/shiba/model.py:296: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(save_location, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 encoding result: {'input_ids': tensor([[57344,    83,    97,  ...,    32,    68,   111]]), 'attention_mask': tensor([[False, False, False,  ..., False, False, False]])}\n",
      "Chunk 1 encoding result: {'input_ids': tensor([[57344,   110,   101,  ...,    97,   109,    46]]), 'attention_mask': tensor([[False, False, False,  ..., False, False, False]])}\n",
      "Decoded Text: Sample PDFThis is a simple PDF ﬁle. Fun fun fun.Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Phasellus facilisis odio sed mi. Curabitur suscipit. Nullam vel nisi. Etiam semper ipsum ut lectus. Proin aliquam, erat eget pharetra commodo, eros mi condimentum quam, sed commodo justo quam ut velit. Integer a erat. Cras laoreet ligula cursus enim. Aenean scelerisque velit et tellus. Vestibulum dictum aliquet sem. Nulla facilisi. Vestibulum accumsan ante vitae elit. Nulla erat dolor, blandit in, rutrum quis, semper pulvinar, enim. Nullam varius congue risus. Vivamus sollicitudin, metus ut interdum eleifend, nisi tellus pellentesque elit, tristique accumsan eros quam et risus. Suspendisse libero odio, mattis sit amet, aliquet eget, hendrerit vel, nulla. Sed vitae augue. Aliquam erat volutpat. Aliquam feugiat vulputate nisl. Suspendisse quis nulla pretium ante pretium mollis. Proin velit ligula, sagittis at, egestas a, pulvinar quis, nisl.Pellentesque sit amet lectus. Praesent pulvinar, nunc quis iaculis sagittis, justo quam lobortis tortor, sed vestibulum dui metus venenatis est. Nunc cursus ligula. Nulla facilisi. Phasellus ullamcorper consectetuer ante. Duis tincidunt, urna id condimentum luctus, nibh ante vulputate sapien, id sagittis massa orci ut enim. Pellentesque vestibulum convallis sem. Nulla consequat quam ut nisl. Nullam est. Curabitur tincidunt dapibus lorem. Proin velit turpis, scelerisque sit amet, iaculis nec, rhoncus ac, ipsum. Phasellus lorem arcu, feugiat eu, gravida eu, consequat molestie, ipsum. Nullam vel est ut ipsum volutpat feugiat. Aenean pellentesque.In mauris. Pellentesque dui nisi, iaculis eu, rhoncus in, venenatis ac, ante. Ut odio justo, scelerisque vel, facilisis non, commodo a, pede. Cras nec massa sit amet tortor volutpat varius. Donec lacinia, neque a luctus aliquet, pede massa imperdiet ante, at varius lorem pede sed sapien. Fusce erat nibh, aliquet in, eleifend eget, commodo eget, erat. Fusce consectetuer. Cras risus tortor, porttitor nec, tristique sed, convallis semper, eros. Fusce vulputate ipsum a mauris. Phasellus mollis. Curabitur sed urna. Aliquam nec sapien non nibh pulvinar convallis. Vivamus facilisis augue quis quam. Proin cursus aliquet metus. Suspendisse lacinia. Nulla at tellus ac turpis eleifend scelerisque. Maecenas a pede vitae enim commodo interdum. Donec odio. Sed sollicitudin dui vitae justo.Morbi elit nunc, facilisis a, mollis a, molestie at, lectus. Suspendisse eget mauris eu tellus molestie cursus. Duis ut magna at justo dignissim condimentum. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Vivamus varius. Ut sit amet diam suscipit mauris ornare aliquam. Sed varius. Duis arcu. Etiam tristique massa eget dui. Phasellus congue. Aenean est erat, tincidunt eget, venenatis quis, commodo at, quam.\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'latin-1' codec can't encode character '\\ue000' in position 50: ordinal not in range(256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecoded Text:\u001b[39m\u001b[38;5;124m\"\u001b[39m, decoded_text)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Save the decoded text as a PDF\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m \u001b[43msave_text_to_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_pdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed text saved as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_pdf_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 65\u001b[0m, in \u001b[0;36msave_text_to_pdf\u001b[0;34m(text, output_pdf_path)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplitlines():\n\u001b[1;32m     63\u001b[0m     pdf\u001b[38;5;241m.\u001b[39mcell(\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m10\u001b[39m, txt\u001b[38;5;241m=\u001b[39mline, ln\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 65\u001b[0m \u001b[43mpdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_pdf_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/fpdf/fpdf.py:1065\u001b[0m, in \u001b[0;36mFPDF.output\u001b[0;34m(self, name, dest)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;66;03m#Finish document if necessary\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m-> 1065\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1066\u001b[0m dest\u001b[38;5;241m=\u001b[39mdest\u001b[38;5;241m.\u001b[39mupper()\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(dest\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/fpdf/fpdf.py:246\u001b[0m, in \u001b[0;36mFPDF.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_endpage()\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m#close document\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_enddoc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/fpdf/fpdf.py:1636\u001b[0m, in \u001b[0;36mFPDF._enddoc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1634\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_enddoc\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1635\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_putheader()\n\u001b[0;32m-> 1636\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_putpages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1637\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_putresources()\n\u001b[1;32m   1638\u001b[0m     \u001b[38;5;66;03m#Info\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/fpdf/fpdf.py:1170\u001b[0m, in \u001b[0;36mFPDF._putpages\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;66;03m#Page content\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompress:\n\u001b[1;32m   1169\u001b[0m     \u001b[38;5;66;03m# manage binary data as latin1 until PEP461 or similar is implemented\u001b[39;00m\n\u001b[0;32m-> 1170\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatin1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m PY3K \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpages[n] \n\u001b[1;32m   1171\u001b[0m     p \u001b[38;5;241m=\u001b[39m zlib\u001b[38;5;241m.\u001b[39mcompress(p)\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'latin-1' codec can't encode character '\\ue000' in position 50: ordinal not in range(256)"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "from fpdf import FPDF\n",
    "from shiba import Shiba, CodepointTokenizer, get_pretrained_from_hub\n",
    "\n",
    "# Load SHIBA model and tokenizer\n",
    "shiba_model = Shiba()\n",
    "shiba_model.load_state_dict(get_pretrained_from_hub())\n",
    "shiba_model.eval()  # Disable dropout\n",
    "tokenizer = CodepointTokenizer()\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a given PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def split_text(text, max_length=1800):\n",
    "    \"\"\"Split text into chunks within the max length allowed by SHIBA.\"\"\"\n",
    "    return [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
    "\n",
    "def process_pdf_with_shiba(pdf_path):\n",
    "    # Extract and clean text from PDF\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    # Split text into manageable chunks\n",
    "    text_chunks = split_text(text)\n",
    "    all_outputs = []\n",
    "    encoded_chunks = []\n",
    "\n",
    "    for i, chunk in enumerate(text_chunks):\n",
    "        # Encode each chunk\n",
    "        encoded = tokenizer.encode_batch([chunk])\n",
    "        print(f\"Chunk {i} encoding result:\", encoded)  # Debugging statement\n",
    "\n",
    "        # Access `input_ids` and `attention_mask` and structure inputs for the model\n",
    "        if 'input_ids' in encoded and 'attention_mask' in encoded:\n",
    "            input_ids = encoded['input_ids']\n",
    "            attention_mask = encoded['attention_mask']\n",
    "            inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "            outputs = shiba_model(**inputs)\n",
    "            all_outputs.append(outputs)\n",
    "            encoded_chunks.append(input_ids[0])  # Store input_ids for decoding\n",
    "\n",
    "    return all_outputs, encoded_chunks\n",
    "\n",
    "def decode_tokens(encoded_chunks):\n",
    "    decoded_text = \"\"\n",
    "    for tokens in encoded_chunks:\n",
    "        decoded_text += tokenizer.decode(tokens)\n",
    "    return decoded_text\n",
    "\n",
    "def save_text_to_pdf(text, output_pdf_path):\n",
    "    \"\"\"Save the provided text to a PDF file.\"\"\"\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "\n",
    "    # Add each line of the text to the PDF, handling line breaks\n",
    "    for line in text.splitlines():\n",
    "        pdf.cell(200, 10, txt=line, ln=True)\n",
    "\n",
    "    pdf.output(output_pdf_path)\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"sample.pdf\"\n",
    "output_pdf_path = \"output.pdf\"\n",
    "outputs, encoded_chunks = process_pdf_with_shiba(pdf_path)\n",
    "decoded_text = decode_tokens(encoded_chunks)\n",
    "print(\"Decoded Text:\", decoded_text)\n",
    "\n",
    "# Save the decoded text as a PDF\n",
    "save_text_to_pdf(decoded_text, output_pdf_path)\n",
    "print(f\"Processed text saved as {output_pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22195b05-2194-45bf-9a62-aae450f9f0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
